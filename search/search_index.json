{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"La majorit\u00e9 des outils d'IA g\u00e9n\u00e9ratrice d'images (Midjourney, ChatGPT, DALL-E) masquent la complexit\u00e9 technique derri\u00e8re une simple barre de texte. Cet atelier propose d'ouvrir la \"bo\u00eete noire\" pour comprendre les m\u00e9canismes fondamentaux qui permettent \u00e0 une machine de transformer des donn\u00e9es num\u00e9riques en images. Le Chemin de la Donn\u00e9e Dans cet atelier, nous allons suivre le parcours d'une id\u00e9e, de votre texte jusqu'aux pixels de l'image finale. graph LR A[Votre Texte] --> B{Le Cerveau IA} B --> C[Image Finale] Les Concepts de Base 1. Le D\u00e9bruitage (Denoising) La g\u00e9n\u00e9ration d'images par diffusion repose sur un processus de r\u00e9duction du bruit. L'IA ne \"dessine\" pas au sens traditionnel ; elle sculpte une information \u00e0 partir d'un chaos de pixels al\u00e9atoires. Le processus en action Visualisation de l'\u00e9mergence d'une forme \u00e0 travers les \u00e9tapes de d\u00e9bruitage. 2. Entra\u00eenement et Nuage de Concepts Durant sa phase d'apprentissage, l'IA n'apprend pas par c\u0153ur des images, mais des relations statistiques entre des mots et des formes. Elle cr\u00e9e un nuage de concepts dans un espace multidimensionnel. Proximit\u00e9 s\u00e9mantique : Les concepts proches (ex: \"chien\" et \"loup\") sont regroup\u00e9s g\u00e9ographiquement. Navigation : G\u00e9n\u00e9rer une image revient \u00e0 demander \u00e0 l'IA de se d\u00e9placer vers une coordonn\u00e9e pr\u00e9cise de ce nuage. Les Composants Techniques 1. L'Espace Latent C'est un espace de travail compress\u00e9 o\u00f9 l'IA calcule l'image. Le VAE (Variational AutoEncoder) est l'outil qui compresse le monde r\u00e9el en \"latents\" et les d\u00e9compresse en pixels visibles. 2. Le Guidage et l'Attention Le texte que vous saisissez est converti en vecteurs par CLIP . Le m\u00e9canisme d'attention permet ensuite \u00e0 l'IA de focaliser ses calculs sur des zones sp\u00e9cifiques de l'image en fonction des mots du prompt. Exemple : Le mot \"chauve\" activera une attention forte sur le sommet du cr\u00e2ne, tandis que \"papiers\" dirigera le calcul vers les objets tenus en main. Structure de l'atelier L'apprentissage est divis\u00e9 en deux modules : Validation des param\u00e8tres : Utilisation de l'interface simplifi\u00e9e LightDiffusion pour tester les r\u00e9glages de base (Seed, Steps, CFG). Architecture nodale : Passage sur ComfyUI avec une approche \"Puzzle\" . Vous devrez reconnecter vous-m\u00eames les composants pour comprendre le chemin des donn\u00e9es. Acc\u00e9der au Module 1 : LightDiffusion \u2192","title":"Introduction"},{"location":"index.html#le-chemin-de-la-donnee","text":"Dans cet atelier, nous allons suivre le parcours d'une id\u00e9e, de votre texte jusqu'aux pixels de l'image finale. graph LR A[Votre Texte] --> B{Le Cerveau IA} B --> C[Image Finale]","title":"Le Chemin de la Donn\u00e9e"},{"location":"index.html#les-concepts-de-base","text":"","title":"Les Concepts de Base"},{"location":"index.html#1-le-debruitage-denoising","text":"La g\u00e9n\u00e9ration d'images par diffusion repose sur un processus de r\u00e9duction du bruit. L'IA ne \"dessine\" pas au sens traditionnel ; elle sculpte une information \u00e0 partir d'un chaos de pixels al\u00e9atoires. Le processus en action Visualisation de l'\u00e9mergence d'une forme \u00e0 travers les \u00e9tapes de d\u00e9bruitage.","title":"1. Le D\u00e9bruitage (Denoising)"},{"location":"index.html#2-entrainement-et-nuage-de-concepts","text":"Durant sa phase d'apprentissage, l'IA n'apprend pas par c\u0153ur des images, mais des relations statistiques entre des mots et des formes. Elle cr\u00e9e un nuage de concepts dans un espace multidimensionnel. Proximit\u00e9 s\u00e9mantique : Les concepts proches (ex: \"chien\" et \"loup\") sont regroup\u00e9s g\u00e9ographiquement. Navigation : G\u00e9n\u00e9rer une image revient \u00e0 demander \u00e0 l'IA de se d\u00e9placer vers une coordonn\u00e9e pr\u00e9cise de ce nuage.","title":"2. Entra\u00eenement et Nuage de Concepts"},{"location":"index.html#les-composants-techniques","text":"","title":"Les Composants Techniques"},{"location":"index.html#1-lespace-latent","text":"C'est un espace de travail compress\u00e9 o\u00f9 l'IA calcule l'image. Le VAE (Variational AutoEncoder) est l'outil qui compresse le monde r\u00e9el en \"latents\" et les d\u00e9compresse en pixels visibles.","title":"1. L'Espace Latent"},{"location":"index.html#2-le-guidage-et-lattention","text":"Le texte que vous saisissez est converti en vecteurs par CLIP . Le m\u00e9canisme d'attention permet ensuite \u00e0 l'IA de focaliser ses calculs sur des zones sp\u00e9cifiques de l'image en fonction des mots du prompt. Exemple : Le mot \"chauve\" activera une attention forte sur le sommet du cr\u00e2ne, tandis que \"papiers\" dirigera le calcul vers les objets tenus en main.","title":"2. Le Guidage et l'Attention"},{"location":"index.html#structure-de-latelier","text":"L'apprentissage est divis\u00e9 en deux modules : Validation des param\u00e8tres : Utilisation de l'interface simplifi\u00e9e LightDiffusion pour tester les r\u00e9glages de base (Seed, Steps, CFG). Architecture nodale : Passage sur ComfyUI avec une approche \"Puzzle\" . Vous devrez reconnecter vous-m\u00eames les composants pour comprendre le chemin des donn\u00e9es. Acc\u00e9der au Module 1 : LightDiffusion \u2192","title":"Structure de l'atelier"},{"location":"advanced.html","text":"Cette page explore des concepts plus techniques pour ceux qui souhaitent approfondir leur compr\u00e9hension des mod\u00e8les de diffusion. Convergence et Samplers Le choix de l'algorithme (Sampler) d\u00e9termine si l'image va se stabiliser (\"converger\") ou continuer \u00e0 \u00e9voluer sans fin \u00e0 chaque \u00e9tape suppl\u00e9mentaire. Euler (Convergent) Euler Ancestral (Non-Convergent) Observation : Les deux exemples utilisent 40 steps. \u00c0 gauche, Euler stabilise l'image. \u00c0 droite, Euler Ancestral (Euler a) continue d'ajouter du bruit \u00e0 chaque \u00e9tape, ce qui emp\u00eache une convergence totale et fait \"bouger\" continuellement les d\u00e9tails. Les Samplers : Euler vs Euler Ancestral Le choix de l'algorithme d'\u00e9chantillonnage (Sampler) influence la fa\u00e7on dont l'image \u00e9volue. Euler : Un \u00e9chantillonneur \"d\u00e9terministe\". Plus vous ajoutez de steps, plus l'image se stabilise vers un r\u00e9sultat fixe. Euler Ancestral (Euler a) : Un \u00e9chantillonneur \"stochastique\". Il ajoute un peu de bruit \u00e0 chaque \u00e9tape, ce qui signifie que le r\u00e9sultat peut continuer \u00e0 changer radicalement m\u00eame avec beaucoup de steps. Pourquoi le Transformer ? Les nouveaux mod\u00e8les comme Flux ou Stable Diffusion 3 remplacent les blocs de convolution par des blocs Transformer (Similaire \u00e0 GPT). Avantage : Une meilleure compr\u00e9hension des structures complexes et une meilleure relation entre les mots du prompt et les objets dans l'image (Prompt Adherence). Performance : Bien que plus gourmands, ils permettent d'atteindre un niveau de d\u00e9tail et de coh\u00e9rence spatiale bien sup\u00e9rieur.","title":"Pour aller plus loin"},{"location":"advanced.html#convergence-et-samplers","text":"Le choix de l'algorithme (Sampler) d\u00e9termine si l'image va se stabiliser (\"converger\") ou continuer \u00e0 \u00e9voluer sans fin \u00e0 chaque \u00e9tape suppl\u00e9mentaire. Euler (Convergent) Euler Ancestral (Non-Convergent) Observation : Les deux exemples utilisent 40 steps. \u00c0 gauche, Euler stabilise l'image. \u00c0 droite, Euler Ancestral (Euler a) continue d'ajouter du bruit \u00e0 chaque \u00e9tape, ce qui emp\u00eache une convergence totale et fait \"bouger\" continuellement les d\u00e9tails.","title":"Convergence et Samplers"},{"location":"advanced.html#les-samplers-euler-vs-euler-ancestral","text":"Le choix de l'algorithme d'\u00e9chantillonnage (Sampler) influence la fa\u00e7on dont l'image \u00e9volue. Euler : Un \u00e9chantillonneur \"d\u00e9terministe\". Plus vous ajoutez de steps, plus l'image se stabilise vers un r\u00e9sultat fixe. Euler Ancestral (Euler a) : Un \u00e9chantillonneur \"stochastique\". Il ajoute un peu de bruit \u00e0 chaque \u00e9tape, ce qui signifie que le r\u00e9sultat peut continuer \u00e0 changer radicalement m\u00eame avec beaucoup de steps.","title":"Les Samplers : Euler vs Euler Ancestral"},{"location":"advanced.html#pourquoi-le-transformer","text":"Les nouveaux mod\u00e8les comme Flux ou Stable Diffusion 3 remplacent les blocs de convolution par des blocs Transformer (Similaire \u00e0 GPT). Avantage : Une meilleure compr\u00e9hension des structures complexes et une meilleure relation entre les mots du prompt et les objets dans l'image (Prompt Adherence). Performance : Bien que plus gourmands, ils permettent d'atteindre un niveau de d\u00e9tail et de coh\u00e9rence spatiale bien sup\u00e9rieur.","title":"Pourquoi le Transformer ?"},{"location":"comfyui.html","text":"ComfyUI est une interface bas\u00e9e sur des graphes de n\u0153uds. Elle permet de visualiser et de manipuler directement le flux de donn\u00e9es (workflow). Cette approche est indispensable pour comprendre comment les composants communiquent entre eux. Terminologie technique N\u0153ud (Node) : Unit\u00e9 de traitement effectuant une op\u00e9ration sp\u00e9cifique (encodage, \u00e9chantillonnage, d\u00e9codage). Liaison (Noodle/Edge) : Connexion transportant un type de donn\u00e9e sp\u00e9cifique (Model, Clip, Latent, VAE). Workflow : L'ensemble du graphe constituant la pipeline de g\u00e9n\u00e9ration. Les composants du workflow standard 1. Load Checkpoint C'est le point d'entr\u00e9e qui charge les poids du mod\u00e8le. Il distribue les donn\u00e9es vers trois flux : MODEL : Transmis au KSampler. CLIP : Transmis aux encodeurs de texte. VAE : Transmis au d\u00e9codeur final. 2. CLIP Text Encode Transforme le texte brut en donn\u00e9es compr\u00e9hensibles par le mod\u00e8le. Ces donn\u00e9es servent de \"guide\" (Conditioning) au processus de d\u00e9bruitage. 3. Empty Latent Image D\u00e9finit les dimensions de sortie et g\u00e9n\u00e8re le bruit initial dans l'espace latent. L'image n'existe pas encore sous forme de pixels \u00e0 cette \u00e9tape. 4. KSampler Le moteur de calcul. Il re\u00e7oit le mod\u00e8le, les prompts (positif/n\u00e9gatif) et le bruit latent. Il effectue les it\u00e9rations de d\u00e9bruitage demand\u00e9es. 5. VAE Decode Prend les donn\u00e9es math\u00e9matiques en sortie du KSampler et utilise le module VAE pour les traduire en pixels affichables. Exercice : Le Puzzle Logique Objectif Dans cet exercice, vous disposez des n\u0153uds n\u00e9cessaires sur votre canevas, mais les c\u00e2bles sont coup\u00e9s ! Vous devez reconnecter les flux en respectant la logique de transport des donn\u00e9es. Utilisez les couleurs des ports pour vous guider. graph LR CP[Load Checkpoint] --- TE[CLIP Text Encode] CP --- KS[KSampler] CP --- VD[VAE Decode] TE --- KS EL[Empty Latent] --- KS KS --- VD VD --- SI[Save Image] Voir la solution (Workflow complet) N'ouvrez ce bloc que si vous \u00eates bloqu\u00e9 ou pour v\u00e9rifier votre travail. Astuces de Pro (Shortcuts) Pour manipuler le puzzle comme un expert : Recherche rapide : Double-cliquez n'importe o\u00f9 sur le fond pour ouvrir le menu de recherche. Clonage : Maintenez ALT et faites glisser un n\u0153ud pour le dupliquer. Aide au branchement : Tirez un c\u00e2ble depuis un point de sortie et l\u00e2chez-le dans le vide ; ComfyUI vous proposera uniquement les n\u0153uds compatibles. [BONUS] Le Puzzle Invers\u00e9 : img2img Si vous avez termin\u00e9 le premier puzzle, essayez de comprendre comment transformer une image existante au lieu de partir d'un canevas vide. Le concept img2img Dans un flux img2img , on remplace le n\u0153ud Empty Latent Image par un duo : Load Image : Pour charger votre fichier. VAE Encode : Pour transformer vos pixels en \"Latent\" (le langage de l'IA). graph LR IMG[Load Image] -- IMAGE --> VE[VAE Encode] CP[Load Checkpoint] -- VAE --> VE VE -- LATENT --> KS[KSampler] style VE fill:#f96,stroke:#333,stroke-width:2px D\u00e9fi : Dans ComfyUI, essayez d'ajouter ces deux n\u0153uds et de les connecter au KSampler. R\u00e9glages recommand\u00e9s Prompt de d\u00e9part : \"A beautiful oil painting, hyper-detailed, masterpiece\" Sampler : Utilisez kl_optimal pour un r\u00e9sultat fluide. Denoise : Commencez \u00e0 0.65 . C'est le \"sweet spot\" pour transformer l'image tout en gardant la structure originale. Synth\u00e8se \u00c0 l'issue de ces modules, vous avez \"ouvert la bo\u00eete noire\". Vous savez maintenant que l'IA ne cr\u00e9e pas d'image ex-nihilo, mais qu'elle sculpte un espace math\u00e9matique (Latent) guid\u00e9e par vos mots (CLIP) et traduite par un d\u00e9codeur (VAE).","title":"ComfyUI"},{"location":"comfyui.html#terminologie-technique","text":"N\u0153ud (Node) : Unit\u00e9 de traitement effectuant une op\u00e9ration sp\u00e9cifique (encodage, \u00e9chantillonnage, d\u00e9codage). Liaison (Noodle/Edge) : Connexion transportant un type de donn\u00e9e sp\u00e9cifique (Model, Clip, Latent, VAE). Workflow : L'ensemble du graphe constituant la pipeline de g\u00e9n\u00e9ration.","title":"Terminologie technique"},{"location":"comfyui.html#les-composants-du-workflow-standard","text":"1. Load Checkpoint C'est le point d'entr\u00e9e qui charge les poids du mod\u00e8le. Il distribue les donn\u00e9es vers trois flux : MODEL : Transmis au KSampler. CLIP : Transmis aux encodeurs de texte. VAE : Transmis au d\u00e9codeur final. 2. CLIP Text Encode Transforme le texte brut en donn\u00e9es compr\u00e9hensibles par le mod\u00e8le. Ces donn\u00e9es servent de \"guide\" (Conditioning) au processus de d\u00e9bruitage. 3. Empty Latent Image D\u00e9finit les dimensions de sortie et g\u00e9n\u00e8re le bruit initial dans l'espace latent. L'image n'existe pas encore sous forme de pixels \u00e0 cette \u00e9tape. 4. KSampler Le moteur de calcul. Il re\u00e7oit le mod\u00e8le, les prompts (positif/n\u00e9gatif) et le bruit latent. Il effectue les it\u00e9rations de d\u00e9bruitage demand\u00e9es. 5. VAE Decode Prend les donn\u00e9es math\u00e9matiques en sortie du KSampler et utilise le module VAE pour les traduire en pixels affichables.","title":"Les composants du workflow standard"},{"location":"comfyui.html#exercice-le-puzzle-logique","text":"Objectif Dans cet exercice, vous disposez des n\u0153uds n\u00e9cessaires sur votre canevas, mais les c\u00e2bles sont coup\u00e9s ! Vous devez reconnecter les flux en respectant la logique de transport des donn\u00e9es. Utilisez les couleurs des ports pour vous guider. graph LR CP[Load Checkpoint] --- TE[CLIP Text Encode] CP --- KS[KSampler] CP --- VD[VAE Decode] TE --- KS EL[Empty Latent] --- KS KS --- VD VD --- SI[Save Image] Voir la solution (Workflow complet) N'ouvrez ce bloc que si vous \u00eates bloqu\u00e9 ou pour v\u00e9rifier votre travail.","title":"Exercice : Le Puzzle Logique"},{"location":"comfyui.html#astuces-de-pro-shortcuts","text":"Pour manipuler le puzzle comme un expert : Recherche rapide : Double-cliquez n'importe o\u00f9 sur le fond pour ouvrir le menu de recherche. Clonage : Maintenez ALT et faites glisser un n\u0153ud pour le dupliquer. Aide au branchement : Tirez un c\u00e2ble depuis un point de sortie et l\u00e2chez-le dans le vide ; ComfyUI vous proposera uniquement les n\u0153uds compatibles.","title":"Astuces de Pro (Shortcuts)"},{"location":"comfyui.html#bonus-le-puzzle-inverse-img2img","text":"Si vous avez termin\u00e9 le premier puzzle, essayez de comprendre comment transformer une image existante au lieu de partir d'un canevas vide. Le concept img2img Dans un flux img2img , on remplace le n\u0153ud Empty Latent Image par un duo : Load Image : Pour charger votre fichier. VAE Encode : Pour transformer vos pixels en \"Latent\" (le langage de l'IA). graph LR IMG[Load Image] -- IMAGE --> VE[VAE Encode] CP[Load Checkpoint] -- VAE --> VE VE -- LATENT --> KS[KSampler] style VE fill:#f96,stroke:#333,stroke-width:2px D\u00e9fi : Dans ComfyUI, essayez d'ajouter ces deux n\u0153uds et de les connecter au KSampler. R\u00e9glages recommand\u00e9s Prompt de d\u00e9part : \"A beautiful oil painting, hyper-detailed, masterpiece\" Sampler : Utilisez kl_optimal pour un r\u00e9sultat fluide. Denoise : Commencez \u00e0 0.65 . C'est le \"sweet spot\" pour transformer l'image tout en gardant la structure originale.","title":"[BONUS] Le Puzzle Invers\u00e9 : img2img"},{"location":"comfyui.html#synthese","text":"\u00c0 l'issue de ces modules, vous avez \"ouvert la bo\u00eete noire\". Vous savez maintenant que l'IA ne cr\u00e9e pas d'image ex-nihilo, mais qu'elle sculpte un espace math\u00e9matique (Latent) guid\u00e9e par vos mots (CLIP) et traduite par un d\u00e9codeur (VAE).","title":"Synth\u00e8se"},{"location":"light-diffusion.html","text":"Cette premi\u00e8re phase utilise l'interface LightDiffusion-Next . L'objectif est de valider les connaissances th\u00e9oriques en observant l'impact direct des param\u00e8tres de g\u00e9n\u00e9ration. Param\u00e8tres de contr\u00f4le Trois variables fondamentales permettent de piloter la g\u00e9n\u00e9ration. Le Prompt (Conditioning) C'est la directive textuelle. L'IA interpr\u00e8te vos mots pour orienter le d\u00e9bruitage. Positif : Les \u00e9l\u00e9ments \u00e0 inclure. N\u00e9gatif : Les \u00e9l\u00e9ments ou styles \u00e0 exclure explicitement. Le Sampling (Steps) Le nombre d'it\u00e9rations que l'IA effectue pour retirer le bruit. Observation : Un nombre trop faible laisse l'image inachev\u00e9e. Un nombre trop \u00e9lev\u00e9 s'av\u00e8re souvent inefficace pass\u00e9 un certain point de convergence. La Seed (D\u00e9terminisme) La Seed est la valeur num\u00e9rique qui initialise le bruit de d\u00e9part. Fixe : Permet de reproduire exactement la m\u00eame image. Al\u00e9atoire (-1) : Produit un nouveau point de d\u00e9part \u00e0 chaque g\u00e9n\u00e9ration. Le CFG Scale (Fid\u00e9lit\u00e9 au Prompt) Le Classifier Free Guidance contr\u00f4le l'\u00e9quilibre entre la cr\u00e9ativit\u00e9 de l'IA et le respect strict de vos instructions. Valeur basse (1-3) : L'IA est tr\u00e8s libre, les couleurs sont souvent d\u00e9lav\u00e9es. Valeur standard (7-9) : Le compromis id\u00e9al. Valeur haute (15+) : L'IA force les traits, les contrastes deviennent extr\u00eames. Exercices d'application Consignes de test Pour chaque exercice, utilisez le mod\u00e8le SD1.5 qui vous est fourni dans le s\u00e9lecteur. Stabilit\u00e9 : G\u00e9n\u00e9rez une image, notez sa Seed, puis relancez. Observez la reproduction. Variabilit\u00e9 : Modifiez un seul adjectif dans votre prompt tout en conservant la m\u00eame Seed. L'effet CFG : Avec une Seed fixe, comparez une g\u00e9n\u00e9ration \u00e0 CFG 1.0, 7.0 et 30.0. Convergence : Observez la diff\u00e9rence de nettet\u00e9 entre 15 et 30 steps. Acc\u00e9der au Module 2 : Architecture nodale (ComfyUI) \u2192 Pour aller plus loin Si vous voulez comprendre les diff\u00e9rences plus subtiles entre les algorithmes (Samplers) ou visualiser la convergence en temps r\u00e9el, consultez la page Pour aller plus loin .","title":"LightDiffusion-Next"},{"location":"light-diffusion.html#parametres-de-controle","text":"Trois variables fondamentales permettent de piloter la g\u00e9n\u00e9ration. Le Prompt (Conditioning) C'est la directive textuelle. L'IA interpr\u00e8te vos mots pour orienter le d\u00e9bruitage. Positif : Les \u00e9l\u00e9ments \u00e0 inclure. N\u00e9gatif : Les \u00e9l\u00e9ments ou styles \u00e0 exclure explicitement. Le Sampling (Steps) Le nombre d'it\u00e9rations que l'IA effectue pour retirer le bruit. Observation : Un nombre trop faible laisse l'image inachev\u00e9e. Un nombre trop \u00e9lev\u00e9 s'av\u00e8re souvent inefficace pass\u00e9 un certain point de convergence. La Seed (D\u00e9terminisme) La Seed est la valeur num\u00e9rique qui initialise le bruit de d\u00e9part. Fixe : Permet de reproduire exactement la m\u00eame image. Al\u00e9atoire (-1) : Produit un nouveau point de d\u00e9part \u00e0 chaque g\u00e9n\u00e9ration. Le CFG Scale (Fid\u00e9lit\u00e9 au Prompt) Le Classifier Free Guidance contr\u00f4le l'\u00e9quilibre entre la cr\u00e9ativit\u00e9 de l'IA et le respect strict de vos instructions. Valeur basse (1-3) : L'IA est tr\u00e8s libre, les couleurs sont souvent d\u00e9lav\u00e9es. Valeur standard (7-9) : Le compromis id\u00e9al. Valeur haute (15+) : L'IA force les traits, les contrastes deviennent extr\u00eames.","title":"Param\u00e8tres de contr\u00f4le"},{"location":"light-diffusion.html#exercices-dapplication","text":"Consignes de test Pour chaque exercice, utilisez le mod\u00e8le SD1.5 qui vous est fourni dans le s\u00e9lecteur. Stabilit\u00e9 : G\u00e9n\u00e9rez une image, notez sa Seed, puis relancez. Observez la reproduction. Variabilit\u00e9 : Modifiez un seul adjectif dans votre prompt tout en conservant la m\u00eame Seed. L'effet CFG : Avec une Seed fixe, comparez une g\u00e9n\u00e9ration \u00e0 CFG 1.0, 7.0 et 30.0. Convergence : Observez la diff\u00e9rence de nettet\u00e9 entre 15 et 30 steps. Acc\u00e9der au Module 2 : Architecture nodale (ComfyUI) \u2192 Pour aller plus loin Si vous voulez comprendre les diff\u00e9rences plus subtiles entre les algorithmes (Samplers) ou visualiser la convergence en temps r\u00e9el, consultez la page Pour aller plus loin .","title":"Exercices d'application"}]}