{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"La majorit\u00e9 des outils d'IA g\u00e9n\u00e9ratrice d'images (Midjourney, ChatGPT, DALL-E) masquent la complexit\u00e9 technique derri\u00e8re une simple barre de texte. Cet atelier propose d'ouvrir la \"bo\u00eete noire\" pour comprendre les m\u00e9canismes fondamentaux qui permettent \u00e0 une machine de transformer des donn\u00e9es num\u00e9riques en repr\u00e9sentations visuelles coh\u00e9rentes. Fonctionnement th\u00e9orique La g\u00e9n\u00e9ration d'images par diffusion repose sur un processus de r\u00e9duction du bruit. Contrairement \u00e0 une croyance commune, l'IA ne \"dessine\" pas ; elle sculpte une information \u00e0 partir d'un chaos statistique. Le concept de d\u00e9bruitage Le processus commence par un bruit gaussien pur (une image compos\u00e9e de pixels al\u00e9atoires). L'IA, entra\u00een\u00e9e sur des millions d'exemples, pr\u00e9dit \u00e0 chaque \u00e9tape la quantit\u00e9 de bruit \u00e0 retirer pour se rapprocher d'un concept connu. Visualisation de l'\u00e9mergence d'une forme \u00e0 travers les \u00e9tapes de d\u00e9bruitage. Composants fondamentaux Pour manipuler ces mod\u00e8les, il est essentiel de comprendre trois composants techniques : 1. L'Espace Latent Travailler sur des images en haute r\u00e9solution pixel par pixel est co\u00fbteux en ressources. La diffusion s'effectue donc dans un espace latent , une version compress\u00e9e et math\u00e9matique de l'image. Le VAE (Variational AutoEncoder) est le composant charg\u00e9 de la compression et de la d\u00e9compression de ces donn\u00e9es. 2. Le Guidage (Conditioning) Le texte que vous saisissez (le prompt) est converti par un encodeur de texte ( CLIP ) en vecteurs math\u00e9matiques. Ces vecteurs agissent comme des contraintes qui orientent le processus de d\u00e9bruitage vers un r\u00e9sultat sp\u00e9cifique. 3. Le Planificateur (Scheduler) Le scheduler d\u00e9termine la vitesse et la m\u00e9thode de retrait du bruit. C'est lui qui d\u00e9finit la trajectoire math\u00e9matique que prendra l'IA pour passer du bruit pur \u00e0 l'image finale. Structure de l'atelier L'apprentissage est divis\u00e9 en deux modules compl\u00e9mentaires : Validation des param\u00e8tres : Utilisation de l'interface simplifi\u00e9e LightDiffusion pour isoler et comprendre l'impact des variables de base (Seed, Steps, CFG). Architecture nodale : Utilisation de ComfyUI pour reconstruire manuellement le flux de donn\u00e9es et comprendre l'interd\u00e9pendance des composants. Acc\u00e9der au Module 1 : LightDiffusion \u2192","title":"Introduction"},{"location":"#fonctionnement-theorique","text":"La g\u00e9n\u00e9ration d'images par diffusion repose sur un processus de r\u00e9duction du bruit. Contrairement \u00e0 une croyance commune, l'IA ne \"dessine\" pas ; elle sculpte une information \u00e0 partir d'un chaos statistique. Le concept de d\u00e9bruitage Le processus commence par un bruit gaussien pur (une image compos\u00e9e de pixels al\u00e9atoires). L'IA, entra\u00een\u00e9e sur des millions d'exemples, pr\u00e9dit \u00e0 chaque \u00e9tape la quantit\u00e9 de bruit \u00e0 retirer pour se rapprocher d'un concept connu. Visualisation de l'\u00e9mergence d'une forme \u00e0 travers les \u00e9tapes de d\u00e9bruitage.","title":"Fonctionnement th\u00e9orique"},{"location":"#composants-fondamentaux","text":"Pour manipuler ces mod\u00e8les, il est essentiel de comprendre trois composants techniques : 1. L'Espace Latent Travailler sur des images en haute r\u00e9solution pixel par pixel est co\u00fbteux en ressources. La diffusion s'effectue donc dans un espace latent , une version compress\u00e9e et math\u00e9matique de l'image. Le VAE (Variational AutoEncoder) est le composant charg\u00e9 de la compression et de la d\u00e9compression de ces donn\u00e9es. 2. Le Guidage (Conditioning) Le texte que vous saisissez (le prompt) est converti par un encodeur de texte ( CLIP ) en vecteurs math\u00e9matiques. Ces vecteurs agissent comme des contraintes qui orientent le processus de d\u00e9bruitage vers un r\u00e9sultat sp\u00e9cifique. 3. Le Planificateur (Scheduler) Le scheduler d\u00e9termine la vitesse et la m\u00e9thode de retrait du bruit. C'est lui qui d\u00e9finit la trajectoire math\u00e9matique que prendra l'IA pour passer du bruit pur \u00e0 l'image finale.","title":"Composants fondamentaux"},{"location":"#structure-de-latelier","text":"L'apprentissage est divis\u00e9 en deux modules compl\u00e9mentaires : Validation des param\u00e8tres : Utilisation de l'interface simplifi\u00e9e LightDiffusion pour isoler et comprendre l'impact des variables de base (Seed, Steps, CFG). Architecture nodale : Utilisation de ComfyUI pour reconstruire manuellement le flux de donn\u00e9es et comprendre l'interd\u00e9pendance des composants. Acc\u00e9der au Module 1 : LightDiffusion \u2192","title":"Structure de l'atelier"},{"location":"comfyui/","text":"ComfyUI est une interface bas\u00e9e sur des graphes de n\u0153uds. Elle permet de visualiser et de manipuler directement le flux de donn\u00e9es (workflow). Cette approche est indispensable pour comprendre comment les composants communiquent entre eux. Terminologie technique N\u0153ud (Node) : Unit\u00e9 de traitement effectuant une op\u00e9ration sp\u00e9cifique (encodage, \u00e9chantillonnage, d\u00e9codage). Liaison (Noodle/Edge) : Connexion transportant un type de donn\u00e9e sp\u00e9cifique (Model, Clip, Latent, VAE). Workflow : L'ensemble du graphe constituant la pipeline de g\u00e9n\u00e9ration. Les composants du workflow standard 1. Load Checkpoint C'est le point d'entr\u00e9e qui charge les poids du mod\u00e8le. Il distribue les donn\u00e9es vers trois flux : MODEL : Transmis au KSampler. CLIP : Transmis aux encodeurs de texte. VAE : Transmis au d\u00e9codeur final. 2. CLIP Text Encode Transforme le texte brut en donn\u00e9es compr\u00e9hensibles par le mod\u00e8le. Ces donn\u00e9es servent de \"guide\" (Conditioning) au processus de d\u00e9bruitage. 3. Empty Latent Image D\u00e9finit les dimensions de sortie et g\u00e9n\u00e8re le bruit initial dans l'espace latent. L'image n'existe pas encore sous forme de pixels \u00e0 cette \u00e9tape. 4. KSampler Le moteur de calcul. Il re\u00e7oit le mod\u00e8le, les prompts (positif/n\u00e9gatif) et le bruit latent. Il effectue les it\u00e9rations de d\u00e9bruitage demand\u00e9es. 5. VAE Decode Prend les donn\u00e9es math\u00e9matiques en sortie du KSampler et utilise le module VAE pour les traduire en pixels affichables. Exercice : Reconstitution du flux Objectif du module Dans cet exercice, vous disposez des n\u0153uds n\u00e9cessaires sur votre canevas, mais les liaisons sont rompues. Vous devez reconnecter les flux en respectant la logique de transport des donn\u00e9es. graph LR CP[Load Checkpoint] --- TE[CLIP Text Encode] CP --- KS[KSampler] CP --- VD[VAE Decode] TE --- KS EL[Empty Latent] --- KS KS --- VD VD --- SI[Save Image] Voir la solution (Workflow complet) N'ouvrez ce bloc que si vous \u00eates bloqu\u00e9 ou pour v\u00e9rifier votre travail. Raccourcis d'interface Double-clic : Ouverture de la recherche rapide de n\u0153uds. Clic droit sur une entr\u00e9e : Permet de convertir un param\u00e8tre (ex: seed) en entr\u00e9e connectable. Glisser-d\u00e9poser depuis un port : Propose automatiquement les n\u0153uds compatibles avec le type de donn\u00e9e. Synth\u00e8se de l'atelier \u00c0 l'issue de ces modules, vous devriez \u00eatre en mesure d'identifier chaque \u00e9tape de la cr\u00e9ation d'une image : de l'intention textuelle au traitement math\u00e9matique dans l'espace latent, jusqu'au d\u00e9codage final.","title":"ComfyUI"},{"location":"comfyui/#terminologie-technique","text":"N\u0153ud (Node) : Unit\u00e9 de traitement effectuant une op\u00e9ration sp\u00e9cifique (encodage, \u00e9chantillonnage, d\u00e9codage). Liaison (Noodle/Edge) : Connexion transportant un type de donn\u00e9e sp\u00e9cifique (Model, Clip, Latent, VAE). Workflow : L'ensemble du graphe constituant la pipeline de g\u00e9n\u00e9ration.","title":"Terminologie technique"},{"location":"comfyui/#les-composants-du-workflow-standard","text":"1. Load Checkpoint C'est le point d'entr\u00e9e qui charge les poids du mod\u00e8le. Il distribue les donn\u00e9es vers trois flux : MODEL : Transmis au KSampler. CLIP : Transmis aux encodeurs de texte. VAE : Transmis au d\u00e9codeur final. 2. CLIP Text Encode Transforme le texte brut en donn\u00e9es compr\u00e9hensibles par le mod\u00e8le. Ces donn\u00e9es servent de \"guide\" (Conditioning) au processus de d\u00e9bruitage. 3. Empty Latent Image D\u00e9finit les dimensions de sortie et g\u00e9n\u00e8re le bruit initial dans l'espace latent. L'image n'existe pas encore sous forme de pixels \u00e0 cette \u00e9tape. 4. KSampler Le moteur de calcul. Il re\u00e7oit le mod\u00e8le, les prompts (positif/n\u00e9gatif) et le bruit latent. Il effectue les it\u00e9rations de d\u00e9bruitage demand\u00e9es. 5. VAE Decode Prend les donn\u00e9es math\u00e9matiques en sortie du KSampler et utilise le module VAE pour les traduire en pixels affichables.","title":"Les composants du workflow standard"},{"location":"comfyui/#exercice-reconstitution-du-flux","text":"Objectif du module Dans cet exercice, vous disposez des n\u0153uds n\u00e9cessaires sur votre canevas, mais les liaisons sont rompues. Vous devez reconnecter les flux en respectant la logique de transport des donn\u00e9es. graph LR CP[Load Checkpoint] --- TE[CLIP Text Encode] CP --- KS[KSampler] CP --- VD[VAE Decode] TE --- KS EL[Empty Latent] --- KS KS --- VD VD --- SI[Save Image] Voir la solution (Workflow complet) N'ouvrez ce bloc que si vous \u00eates bloqu\u00e9 ou pour v\u00e9rifier votre travail. Raccourcis d'interface Double-clic : Ouverture de la recherche rapide de n\u0153uds. Clic droit sur une entr\u00e9e : Permet de convertir un param\u00e8tre (ex: seed) en entr\u00e9e connectable. Glisser-d\u00e9poser depuis un port : Propose automatiquement les n\u0153uds compatibles avec le type de donn\u00e9e.","title":"Exercice : Reconstitution du flux"},{"location":"comfyui/#synthese-de-latelier","text":"\u00c0 l'issue de ces modules, vous devriez \u00eatre en mesure d'identifier chaque \u00e9tape de la cr\u00e9ation d'une image : de l'intention textuelle au traitement math\u00e9matique dans l'espace latent, jusqu'au d\u00e9codage final.","title":"Synth\u00e8se de l'atelier"},{"location":"light-diffusion/","text":"Cette premi\u00e8re phase utilise l'interface LightDiffusion-Next . L'objectif est de valider les connaissances th\u00e9oriques en observant l'impact direct des param\u00e8tres de g\u00e9n\u00e9ration sur une interface \u00e9pur\u00e9e. Param\u00e8tres de contr\u00f4le Trois variables fondamentales permettent de piloter la g\u00e9n\u00e9ration. Le Prompt (Conditioning) C'est la directive textuelle. L'IA interpr\u00e8te vos mots pour orienter le d\u00e9bruitage. Positif : Les \u00e9l\u00e9ments \u00e0 inclure. N\u00e9gatif : Les \u00e9l\u00e9ments ou styles \u00e0 exclure explicitement. Le Sampling (Steps) Le nombre d'it\u00e9rations que l'IA effectue pour retirer le bruit. Observation : Un nombre trop faible (ex: 10) laisse l'image inachev\u00e9e. Un nombre trop \u00e9lev\u00e9 (ex: 50) s'av\u00e8re souvent inefficace pass\u00e9 un certain point de convergence. La Seed (D\u00e9terminisme) La Seed est la valeur num\u00e9rique qui initialise le bruit de d\u00e9part. Fixe : Permet de reproduire exactement la m\u00eame image ou de tester l'influence d'un changement de texte sur une base identique. Al\u00e9atoire (-1) : Produit un nouveau point de d\u00e9part \u00e0 chaque g\u00e9n\u00e9ration. Le CFG Scale (Fid\u00e9lit\u00e9 au Prompt) Le Classifier Free Guidance contr\u00f4le l'\u00e9quilibre entre la cr\u00e9ativit\u00e9 de l'IA et le respect strict de vos instructions. Valeur basse (1-3) : L'IA est tr\u00e8s libre, les couleurs sont souvent d\u00e9lav\u00e9es et le prompt est peu suivi. Valeur standard (7-9) : Le compromis id\u00e9al pour la plupart des mod\u00e8les. Valeur haute (15+) : L'IA force les traits, les contrastes deviennent extr\u00eames (\"deep fried\") et des artefacts peuvent appara\u00eetre. Le Sampler (M\u00e9thode de calcul) C'est l'algorithme qui choisit comment retirer le bruit. Certains convergent tr\u00e8s vite (Euler a, UniPC), d'autres demandent plus de temps mais offrent une texture plus fine (DPM++ 2M SDE). Exercices d'application Consignes de test Pour chaque exercice, utilisez un mod\u00e8le (checkpoint) de type SDXL ou SD1.5 disponible dans le s\u00e9lecteur. Stabilit\u00e9 : G\u00e9n\u00e9rez une image, notez sa Seed, puis relancez la g\u00e9n\u00e9ration. Observez la reproduction identique. Variabilit\u00e9 : Modifiez un seul adjectif dans votre prompt tout en conservant la m\u00eame Seed. Analysez comment l'IA adapte la structure existante au nouveau concept. L'effet CFG : Avec une Seed fixe, comparez une g\u00e9n\u00e9ration \u00e0 CFG 1.0, 7.0 et 30.0. Observez la d\u00e9gradation de l'image aux extr\u00eames. Comparaison de Samplers : Testez le m\u00eame prompt et la m\u00eame Seed avec Euler a puis avec DPM++ 2M Karras . Notez les diff\u00e9rences de d\u00e9tails, notamment sur les textures complexes. Convergence : Observez la diff\u00e9rence de nettet\u00e9 entre 15 et 30 steps sur un m\u00eame prompt. Acc\u00e9der au Module 2 : Architecture nodale (ComfyUI) \u2192","title":"LightDiffusion-Next"},{"location":"light-diffusion/#parametres-de-controle","text":"Trois variables fondamentales permettent de piloter la g\u00e9n\u00e9ration. Le Prompt (Conditioning) C'est la directive textuelle. L'IA interpr\u00e8te vos mots pour orienter le d\u00e9bruitage. Positif : Les \u00e9l\u00e9ments \u00e0 inclure. N\u00e9gatif : Les \u00e9l\u00e9ments ou styles \u00e0 exclure explicitement. Le Sampling (Steps) Le nombre d'it\u00e9rations que l'IA effectue pour retirer le bruit. Observation : Un nombre trop faible (ex: 10) laisse l'image inachev\u00e9e. Un nombre trop \u00e9lev\u00e9 (ex: 50) s'av\u00e8re souvent inefficace pass\u00e9 un certain point de convergence. La Seed (D\u00e9terminisme) La Seed est la valeur num\u00e9rique qui initialise le bruit de d\u00e9part. Fixe : Permet de reproduire exactement la m\u00eame image ou de tester l'influence d'un changement de texte sur une base identique. Al\u00e9atoire (-1) : Produit un nouveau point de d\u00e9part \u00e0 chaque g\u00e9n\u00e9ration. Le CFG Scale (Fid\u00e9lit\u00e9 au Prompt) Le Classifier Free Guidance contr\u00f4le l'\u00e9quilibre entre la cr\u00e9ativit\u00e9 de l'IA et le respect strict de vos instructions. Valeur basse (1-3) : L'IA est tr\u00e8s libre, les couleurs sont souvent d\u00e9lav\u00e9es et le prompt est peu suivi. Valeur standard (7-9) : Le compromis id\u00e9al pour la plupart des mod\u00e8les. Valeur haute (15+) : L'IA force les traits, les contrastes deviennent extr\u00eames (\"deep fried\") et des artefacts peuvent appara\u00eetre. Le Sampler (M\u00e9thode de calcul) C'est l'algorithme qui choisit comment retirer le bruit. Certains convergent tr\u00e8s vite (Euler a, UniPC), d'autres demandent plus de temps mais offrent une texture plus fine (DPM++ 2M SDE).","title":"Param\u00e8tres de contr\u00f4le"},{"location":"light-diffusion/#exercices-dapplication","text":"Consignes de test Pour chaque exercice, utilisez un mod\u00e8le (checkpoint) de type SDXL ou SD1.5 disponible dans le s\u00e9lecteur. Stabilit\u00e9 : G\u00e9n\u00e9rez une image, notez sa Seed, puis relancez la g\u00e9n\u00e9ration. Observez la reproduction identique. Variabilit\u00e9 : Modifiez un seul adjectif dans votre prompt tout en conservant la m\u00eame Seed. Analysez comment l'IA adapte la structure existante au nouveau concept. L'effet CFG : Avec une Seed fixe, comparez une g\u00e9n\u00e9ration \u00e0 CFG 1.0, 7.0 et 30.0. Observez la d\u00e9gradation de l'image aux extr\u00eames. Comparaison de Samplers : Testez le m\u00eame prompt et la m\u00eame Seed avec Euler a puis avec DPM++ 2M Karras . Notez les diff\u00e9rences de d\u00e9tails, notamment sur les textures complexes. Convergence : Observez la diff\u00e9rence de nettet\u00e9 entre 15 et 30 steps sur un m\u00eame prompt. Acc\u00e9der au Module 2 : Architecture nodale (ComfyUI) \u2192","title":"Exercices d'application"}]}